{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 - NLP Practices for basic.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rinQ6qH9q1x-",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization\n",
        "\n",
        "**Tokenization :** Given a character sequence and a defined document unit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34uAqLVBqSC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY1STyUCsWdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVtGGKEYsfYw",
        "colab_type": "text"
      },
      "source": [
        "## Solve the examples from the slides using spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G28692rsWYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "example1 = nlp(\"This is an example of text tokenization\")\n",
        "\n",
        "for token in example1:\n",
        "    print(token.text)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67NUVH_xsWVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example2 = nlp(\"The quick brown fox jumps over the lazy dog\")\n",
        "\n",
        "for token in example2:\n",
        "    print(token.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-dQsyYHsWSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example3 = nlp(\"We’re the champions\")\n",
        "\n",
        "for token in example3:\n",
        "    print(token.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuqi_ABKsWQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example4 = nlp(\"Will we have dinner today?\")\n",
        "\n",
        "for token in example4:\n",
        "    print(token.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAPE_JZyt0JC",
        "colab_type": "text"
      },
      "source": [
        "# Stemming\n",
        "\n",
        "**Stemming**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6fwK2Rot8FY",
        "colab_type": "text"
      },
      "source": [
        "## Solving the Stemming examples from the slides using NLTK\n",
        "## More examples added in this notebook\n",
        "### You can install NLTK from https://www.nltk.org/install.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfVXpe-0sWOy",
        "colab_type": "code",
        "outputId": "332ee60d-1e57-498e-d468-c7dbfe88a855",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlG_74GksWFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysWLWYpiuZci",
        "colab_type": "code",
        "outputId": "226d0beb-cbae-4e54-b120-210335c582c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example = \"Cats Running Was\"\n",
        "example = [stemmer.stem(token) for token in example.split(\" \")]\n",
        "print(\" \".join(example))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat run wa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXmAFBAcuZZq",
        "colab_type": "code",
        "outputId": "c8c7bb5d-7c76-46b4-d62d-08da861a48d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "lyrics = \"You better lose yourself in the music, the moment \"\\\n",
        "+ \"You own it, you better never let it go \"\\\n",
        "+ \"You only get one shot, do not miss your chance to blow \"\\\n",
        "+ \"This opportunity comes once in a lifetime \"\n",
        "lyrics = [stemmer.stem(token) for token in lyrics.split(\" \")]\n",
        "print(\" \".join(lyrics))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you better lose yourself in the music, the moment you own it, you better never let it go you onli get one shot, do not miss your chanc to blow thi opportun come onc in a lifetim \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27ZyQBpAuZWT",
        "colab_type": "code",
        "outputId": "a0461300-f2af-47df-bccf-c7d0b0760646",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "review = \"Bromwell High is a cartoon comedy. \"\\\n",
        "+ \"It ran at the same time as some other programs about school life, such as \\\"Teachers\\\". \"\\\n",
        "+ \"My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much \"\\\n",
        "+ \"closer to reality than is \\\"Teachers\\\". The scramble to survive financially, the insightful \"\\\n",
        "+ \"students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation \"\\\n",
        "+ \", all remind me of the schools I knew and their students. When I saw the episode in which a student \"\\\n",
        "+ \"repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. \"\\\n",
        "+ \"A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. \"\\\n",
        "+ \"I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\"\n",
        "\n",
        "review= [stemmer.stem(token) for token in review.split(\" \")]\n",
        "print(\" \".join(review))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bromwel high is a cartoon comedy. It ran at the same time as some other program about school life, such as \"teachers\". My 35 year in the teach profess lead me to believ that bromwel high' satir is much closer to realiti than is \"teachers\". the scrambl to surviv financially, the insight student who can see right through their pathet teachers' pomp, the petti of the whole situat , all remind me of the school I knew and their students. when I saw the episod in which a student repeatedli tri to burn down the school, I immedi recal ......... at .......... high. A classic line: inspector: i'm here to sack one of your teachers. student: welcom to bromwel high. I expect that mani adult of my age think that bromwel high is far fetched. what a piti that it isn't!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCCSoetP8Joi",
        "colab_type": "text"
      },
      "source": [
        "# Lemmatization\n",
        "\n",
        "## In this notebook we solve the examples from the slides using spaCy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmjfRnVG5glc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5oVeZJ75gjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC4HglKo5gcx",
        "colab_type": "code",
        "outputId": "2e8d8b0c-1366-497d-d9db-e44075d83142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example1 = nlp(\"Animals\")\n",
        "for token in example1:\n",
        "    print(token.lemma_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "animal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wzB_m655gYv",
        "colab_type": "code",
        "outputId": "9b8e22cb-7536-455d-a527-1f8479703ab3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "example2 = nlp(\"is am are\")\n",
        "for token in example2:\n",
        "    print(token.lemma_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "be\n",
            "be\n",
            "be\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JMz1btW5gTE",
        "colab_type": "code",
        "outputId": "de7f2ecd-a579-4b8a-82a0-1ae6fb620712",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "lyrics = \"You better lose yourself in the music, the moment \"\\\n",
        "+ \"You own it, you better never let it go \" \\\n",
        "+ \"You only get one shot, do not miss your chance to blow \"\\\n",
        "+ \"This opportunity comes once in a lifetime\"\n",
        "\n",
        "example3 = nlp(lyrics)\n",
        "\n",
        "for token in example3:\n",
        "    print(token.lemma_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-PRON-\n",
            "better\n",
            "lose\n",
            "-PRON-\n",
            "in\n",
            "the\n",
            "music\n",
            ",\n",
            "the\n",
            "moment\n",
            "-PRON-\n",
            "own\n",
            "-PRON-\n",
            ",\n",
            "-PRON-\n",
            "better\n",
            "never\n",
            "let\n",
            "-PRON-\n",
            "go\n",
            "-PRON-\n",
            "only\n",
            "get\n",
            "one\n",
            "shot\n",
            ",\n",
            "do\n",
            "not\n",
            "miss\n",
            "-PRON-\n",
            "chance\n",
            "to\n",
            "blow\n",
            "this\n",
            "opportunity\n",
            "come\n",
            "once\n",
            "in\n",
            "a\n",
            "lifetime\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktal-lAS-RIG",
        "colab_type": "text"
      },
      "source": [
        "# Vectorization\n",
        "## In this notebook we solve the examples in the slides and more using scikit-learn\n",
        "## You can install scikit-learn from: http://scikit-learn.org/stable/install.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSt5FQtw-Sv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(binary=True, token_pattern=r'\\b[^\\d\\W]+\\b')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYfxDxUZ-SsW",
        "colab_type": "code",
        "outputId": "f0a27c9b-1362-407e-e1c7-069f06d16c07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "corpus = [\"The dog is on the table\", \"the cats now are on the table\"]\n",
        "vectorizer.fit(corpus)\n",
        "print(vectorizer.transform([\"The dog is on the table\"]).toarray())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 1 1 0 1 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsJyVeiV-SqL",
        "colab_type": "code",
        "outputId": "349cb65e-125b-4cdf-df49-7aaf63ba0857",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "vocab = vectorizer.vocabulary_\n",
        "\n",
        "for key in sorted(vocab.keys()):\n",
        "    print(\"{}: {}\".format(key, vocab[key]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "are: 0\n",
            "cats: 1\n",
            "dog: 2\n",
            "is: 3\n",
            "now: 4\n",
            "on: 5\n",
            "table: 6\n",
            "the: 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck4ljufE-Sn7",
        "colab_type": "code",
        "outputId": "f49e5dc9-ecf1-45e8-8675-c32fc66b4d0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "corpus2 = [\"I am jack\", \"You are john\", \"I am john\"]\n",
        "vectorizer.fit(corpus2)\n",
        "\n",
        "print(vectorizer.transform(corpus2).toarray())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 1 1 0 0]\n",
            " [0 1 0 0 1 1]\n",
            " [1 0 1 0 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ7U0mnM-SlZ",
        "colab_type": "code",
        "outputId": "75b44e1b-b5ee-4e3b-d502-831a89eb6378",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "vocab = vectorizer.vocabulary_\n",
        "\n",
        "for key in sorted(vocab.keys()):\n",
        "    print(\"{}: {}\".format(key, vocab[key]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "am: 0\n",
            "are: 1\n",
            "i: 2\n",
            "jack: 3\n",
            "john: 4\n",
            "you: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQCE5_OhCPBk",
        "colab_type": "text"
      },
      "source": [
        "# Word2vec\n",
        "## In this notebook we will play with spaCy's word vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCedHD-e-Si0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "#nlp = spacy.load('en_core_web_lg')\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14fmjEJT-Sbk",
        "colab_type": "code",
        "outputId": "183247c1-024b-4e75-b487-4cf43632a61c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "example1 = \"man woman king queen\"\n",
        "tokens = nlp(example1)\n",
        "for token1 in tokens:\n",
        "    for token2 in tokens:\n",
        "        print(token1.text, token2.text, token1.similarity(token2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "man man 1.0\n",
            "man woman 0.63921684\n",
            "man king 0.5535262\n",
            "man queen 0.18746983\n",
            "woman man 0.63921684\n",
            "woman woman 1.0\n",
            "woman king 0.6757708\n",
            "woman queen 0.26638454\n",
            "king man 0.5535262\n",
            "king woman 0.6757708\n",
            "king king 1.0\n",
            "king queen 0.3645325\n",
            "queen man 0.18746983\n",
            "queen woman 0.26638454\n",
            "queen king 0.3645325\n",
            "queen queen 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ9dyvn4PWUS",
        "colab_type": "code",
        "outputId": "538f1c0d-fea4-41d9-a1b4-a9dc7db155cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "example1 = \"walking walked swimming swam\"\n",
        "tokens = nlp(example1)\n",
        "for token1 in tokens:\n",
        "    for token2 in tokens:\n",
        "        if(token1.text == token2.text):\n",
        "            continue\n",
        "        print(token1.text, token2.text, token1.similarity(token2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "walking walked 0.11502897\n",
            "walking swimming 0.7308439\n",
            "walking swam 0.15747786\n",
            "walked walking 0.11502897\n",
            "walked swimming 0.13930066\n",
            "walked swam 0.09911855\n",
            "swimming walking 0.7308439\n",
            "swimming walked 0.13930066\n",
            "swimming swam 0.24586298\n",
            "swam walking 0.15747786\n",
            "swam walked 0.09911855\n",
            "swam swimming 0.24586298\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trNkQr8oPWt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example1 = \"spain russia madrid moscow\"\n",
        "tokens = nlp(example1)\n",
        "for token1 in tokens:\n",
        "    for token2 in tokens:\n",
        "        if(token1.text == token2.text):\n",
        "            continue\n",
        "        print(token1.text, token2.text, token1.similarity(token2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36wd0BgrPWrw",
        "colab_type": "code",
        "outputId": "82e0282e-846e-443d-d70f-2a08a22a2f02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "example1 = \"cat dog\"\n",
        "tokens = nlp(example1)\n",
        "for token1 in tokens:\n",
        "    for token2 in tokens:\n",
        "        if(token1.text == token2.text):\n",
        "            continue        \n",
        "        print(token1.text, token2.text, token1.similarity(token2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat dog 0.44518518\n",
            "dog cat 0.44518518\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gZccG3TPWpu",
        "colab_type": "code",
        "outputId": "79d1c2f3-2baa-450d-f2f3-f624c1c23533",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "example1 = \"cat pizza\"\n",
        "tokens = nlp(example1)\n",
        "for token1 in tokens:\n",
        "    for token2 in tokens:\n",
        "        if(token1.text == token2.text):\n",
        "            continue        \n",
        "        print(token1.text, token2.text, token1.similarity(token2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat pizza 0.3983068\n",
            "pizza cat 0.3983068\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvY8-fDqPWgV",
        "colab_type": "code",
        "outputId": "f8403103-fc9f-4eb4-f354-8e971c269890",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "example1 = \"flower pasta\"\n",
        "tokens = nlp(example1)\n",
        "for token1 in tokens:\n",
        "    for token2 in tokens:\n",
        "        if(token1.text == token2.text):\n",
        "            continue        \n",
        "        print(token1.text, token2.text, token1.similarity(token2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "flower pasta 0.4998248\n",
            "pasta flower 0.4998248\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWfAML4FQMYF",
        "colab_type": "text"
      },
      "source": [
        "# Named Entity Recognition\n",
        "## In this notebook we will explore spaCy's abilities at detecting named entities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpBbm1SWQNHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXlTMStdQNNI",
        "colab_type": "code",
        "outputId": "9b937223-cc92-43ff-a66a-a8220a53e92f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "example = \"Google, a company founded by Larry Page and Sergey Brin in the United States of America \"\\\n",
        "+ \"has one of the world’s most advanced search engines.\"\n",
        "\n",
        "doc = nlp(example)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Google ORG\n",
            "Larry Page PERSON\n",
            "Sergey Brin PERSON\n",
            "the United States of America GPE\n",
            "one CARDINAL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bApdV8KbR87j",
        "colab_type": "code",
        "outputId": "a22ceaef-863b-4521-9411-01c57f6ed754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "example = \"The company's rapid growth since incorporation has triggered a chain of products, acquisitions, and partnerships beyond Google's core search engine (Google Search). It offers services designed for work and productivity (Google Docs, Google Sheets, and Google Slides), email (Gmail), scheduling and time management (Google Calendar), cloud storage (Google Drive), instant messaging and video chat (Duo, Hangouts), language translation (Google Translate), mapping and navigation (Google Maps, Waze, Google Earth, Street View), video sharing (YouTube), note-taking (Google Keep), and photo organizing and editing (Google Photos). The company leads the development of the Android mobile operating system, the Google Chrome web browser, and Chrome OS, a lightweight operating system based on the Chrome browser. Google has moved increasingly into hardware; from 2010 to 2015, it partnered with major electronics manufacturers in the production of its Nexus devices, and it released multiple hardware products in October 2016, including the Google Pixel smartphone\"\n",
        "doc = nlp(example)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Google ORG\n",
            "Google Search ORG\n",
            "Google Docs ORG\n",
            "Google Sheets ORG\n",
            "Google Slides ORG\n",
            "Gmail PERSON\n",
            "Google Calendar ORG\n",
            "Google Drive ORG\n",
            "Duo PERSON\n",
            "Hangouts NORP\n",
            "Google Translate ORG\n",
            "Google Maps PERSON\n",
            "Waze PERSON\n",
            "Google Earth PERSON\n",
            "Street View FAC\n",
            "Google Keep PERSON\n",
            "Google ORG\n",
            "Android ORG\n",
            "Google Chrome PRODUCT\n",
            "Chrome OS ORG\n",
            "Chrome ORG\n",
            "Google ORG\n",
            "2010 DATE\n",
            "2015 DATE\n",
            "Nexus ORG\n",
            "October 2016 DATE\n",
            "Google Pixel ORG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqIrwmmsRJbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example = \"Google, a company founded by Larry Page and Sergey Brin in the United States of America \"\\\n",
        "+ \"has one of the world’s most advanced search engines.\"\n",
        "\n",
        "doc = nlp(example)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrTnGUpnQNEf",
        "colab_type": "code",
        "outputId": "1a537775-f47b-46e2-a9dc-97d0aad3dd21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "example = \"U.S. officials are meeting with former Taliban members \"\\\n",
        "+ \"amid intensifying efforts to wind down America's longest war, three of the \"\\\n",
        "+ \"militant group's commanders told NBC News.\"\n",
        "\n",
        "doc = nlp(example)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "U.S. GPE\n",
            "Taliban ORG\n",
            "America GPE\n",
            "three CARDINAL\n",
            "NBC News ORG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XClENprYQNB7",
        "colab_type": "code",
        "outputId": "c5034b79-f2d5-4b9f-822b-d25b8ba04cda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "example = \"It’s been an arduous year for German chancellor Angela Merkel, so far. \"\\\n",
        "+ \"She has battled through coalition negotiations to form a government, chivvied \"\\\n",
        "+ \"the European Union into a loose agreement on migrants, weathered insults from \"\\\n",
        "+ \"US president Donald Trump, and headed off a revolt from her interior minister. No wonder \"\\\n",
        "+ \"then one journalist at her summer news conference in Berlin today (July 20) asked if \"\\\n",
        "+ \"she was, honestly, just exhausted. “I can’t complain,” Merkel said, “I have a few days \"\\\n",
        "+ \"holiday now and am looking forward to sleeping a bit longer.”\"\n",
        "\n",
        "doc = nlp(example)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "an arduous year DATE\n",
            "German NORP\n",
            "Angela Merkel PERSON\n",
            "the European Union ORG\n",
            "US GPE\n",
            "Donald Trump PERSON\n",
            "summer DATE\n",
            "Berlin GPE\n",
            "today DATE\n",
            "July 20 DATE\n",
            "Merkel ORG\n",
            "a few days holiday DATE\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}